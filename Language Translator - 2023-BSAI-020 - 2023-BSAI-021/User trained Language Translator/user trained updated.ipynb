{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0238858-d530-476a-bd36-9deb2d5959db",
   "metadata": {},
   "source": [
    "# User Trained Language Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406ade6-0a81-4f2c-80a3-92c908d92587",
   "metadata": {},
   "source": [
    "This model is a multilingual text translation system using a sequence-to-sequence architecture with LSTM layers. It preprocesses text data by tokenizing and padding sequences, then trains an encoder-decoder model to translate between languages. The model predicts translations word by word and uses a Tkinter-based GUI for user interaction. Users can input a sentence, select languages, and view the translation along with training metrics (loss and accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b00a8-e0a8-4fec-8ef3-d57ac7724a2f",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2acf3fc6-a2d7-42a7-ad13-e77f48351d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e0044-700f-4dca-8d2a-e4cb2251d393",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b084bbd-dc93-4ebe-bc61-f56e6b51cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"lt dataset updated.csv\")\n",
    "\n",
    "# List of available languages\n",
    "languages = data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041ccbc-c5db-4f53-a41a-680847fe3ad2",
   "metadata": {},
   "source": [
    "## Creating and Fitting Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31837600-b3ef-436b-bb93-1a4295d8d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data and create tokenizers for each language\n",
    "tokenizers = {}\n",
    "sequences = {}\n",
    "max_lengths = {}\n",
    "\n",
    "for language in languages:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizers[language] = tokenizer\n",
    "    texts = data[language].values\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences[language] = tokenizer.texts_to_sequences(texts)\n",
    "    max_lengths[language] = max(len(seq) for seq in sequences[language])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1fba-e376-42d9-a88c-b867f5400083",
   "metadata": {},
   "source": [
    "## Padding Sequences to Ensure Equal Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00164f0-fd50-4c26-882e-df08e111fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to make all sequences the same length\n",
    "padded_sequences = {}\n",
    "for language in languages:\n",
    "    padded_sequences[language] = pad_sequences(sequences[language], maxlen=max_lengths[language], padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba9ed0-4851-4d3f-afd2-ec9a97a7de7b",
   "metadata": {},
   "source": [
    "## Preparing Target Sequences with Start and End Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60b6c03-dd73-4dc5-acb2-bfe73c2f237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create target sequences with start and end tokens\n",
    "def prepare_target_sequences(target_texts, tokenizer, max_len):\n",
    "    target_texts = ['<start> ' + text + ' <end>' for text in target_texts]\n",
    "    target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "    return pad_sequences(target_sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaad3c-cce8-4d3c-a877-700bb8a475fa",
   "metadata": {},
   "source": [
    "## Defining Source and Target Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f3f8c5-aaf1-4ef4-8a66-76d70ee98fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and target languages\n",
    "source_language = \"English\"  # Replace with a dynamic choice if needed\n",
    "target_language = \"French\"   # Replace with a dynamic choice if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dcfd4-ad30-4976-9b41-f171db5b56c2",
   "metadata": {},
   "source": [
    "## Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ccd631-7370-4866-836f-1b4b335c0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "source_seq = padded_sequences[source_language]\n",
    "target_texts = data[target_language].values\n",
    "target_seq = prepare_target_sequences(target_texts, tokenizers[target_language], max_lengths[target_language])\n",
    "\n",
    "# Prepare the target data for training (shifted sequences)\n",
    "target_seq_input = np.zeros_like(target_seq)\n",
    "target_seq_input[:, 1:] = target_seq[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb001d-9128-43d0-a9c1-eb4157fdad5e",
   "metadata": {},
   "source": [
    "## Defining the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c390ec8a-e148-4af2-8c61-fbd02caa52d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define vocabulary sizes\n",
    "source_vocab_size = len(tokenizers[source_language].word_index) + 1\n",
    "target_vocab_size = len(tokenizers[target_language].word_index) + 1\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(source_seq, target_seq_input, test_size=0.2)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Encoder\n",
    "model.add(Embedding(input_dim=source_vocab_size, output_dim=256, input_length=max_lengths[source_language]))\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# Decoder\n",
    "model.add(RepeatVector(max_lengths[target_language]))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(target_vocab_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e4726-c88a-4d07-8c78-8530a65e8cde",
   "metadata": {},
   "source": [
    "## Compiling and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0439e4c4-70da-40ec-ab56-46d9c1d3cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model, please wait...\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 255ms/step - accuracy: 0.6123 - loss: 3.3578 - val_accuracy: 0.6155 - val_loss: 2.8386\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 254ms/step - accuracy: 0.6217 - loss: 2.5604 - val_accuracy: 0.6584 - val_loss: 2.3339\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 254ms/step - accuracy: 0.6587 - loss: 2.2874 - val_accuracy: 0.6662 - val_loss: 2.2729\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 254ms/step - accuracy: 0.6702 - loss: 2.2043 - val_accuracy: 0.6740 - val_loss: 2.1914\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 254ms/step - accuracy: 0.6817 - loss: 2.0967 - val_accuracy: 0.6871 - val_loss: 2.0651\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 253ms/step - accuracy: 0.6993 - loss: 1.9451 - val_accuracy: 0.7062 - val_loss: 1.8990\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 254ms/step - accuracy: 0.7202 - loss: 1.7613 - val_accuracy: 0.7213 - val_loss: 1.7639\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 254ms/step - accuracy: 0.7367 - loss: 1.5990 - val_accuracy: 0.7341 - val_loss: 1.6446\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 253ms/step - accuracy: 0.7493 - loss: 1.4596 - val_accuracy: 0.7447 - val_loss: 1.5566\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 254ms/step - accuracy: 0.7605 - loss: 1.3261 - val_accuracy: 0.7499 - val_loss: 1.4862\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Can be 'val_accuracy' if you prefer\n",
    "    patience=3,          # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore the best model weights when stopping\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "print(\"Training the model, please wait...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    np.expand_dims(y_train, -1),\n",
    "    epochs=10,  # Adjust epochs as needed for your dataset\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, np.expand_dims(y_test, -1)),\n",
    "    callbacks=[early_stopping]  # Add early stopping callback here\n",
    ")\n",
    "print(\"Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ce17899-9dac-4903-8f2b-79fd2d4bdf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# print(\"Training the model, please wait...\")\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     np.expand_dims(y_train, -1),\n",
    "#     epochs=20,  # Adjust epochs as needed for your dataset\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_test, np.expand_dims(y_test, -1))\n",
    "# )\n",
    "# print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198622d-e225-408d-86b2-7c2daf0d68d5",
   "metadata": {},
   "source": [
    "## Evaluating the Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48a2e64f-a7d2-41ed-ac02-fd68d9037dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total loss and accuracy\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_accuracy = history.history['accuracy'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e63aeb-2cc7-483d-85ae-550fc4a6ab9a",
   "metadata": {},
   "source": [
    "## Creating the Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40f64647-57d8-4c8b-8c07-5b2eebc4f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a sentence\n",
    "def translate_sentence(sentence, src_lang, tgt_lang):\n",
    "    source_tokenizer = tokenizers[src_lang]\n",
    "    target_tokenizer = tokenizers[tgt_lang]\n",
    "    \n",
    "    sequence = source_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_lengths[src_lang], padding='post')\n",
    "    predicted_seq = model.predict(padded_sequence)\n",
    "    \n",
    "    translated_words = []\n",
    "    for word_probs in predicted_seq[0]:\n",
    "        word_index = np.argmax(word_probs)\n",
    "        \n",
    "        if word_index == 0:  # Padding token\n",
    "            continue\n",
    "        if word_index == target_tokenizer.word_index.get('<end>', -1):\n",
    "            break\n",
    "        if word_index == target_tokenizer.word_index.get('<start>', -1):\n",
    "            continue\n",
    "        \n",
    "        word = target_tokenizer.index_word.get(word_index, None)\n",
    "        if word is None:  # Handle unknown token\n",
    "            break\n",
    "        \n",
    "        translated_words.append(word)\n",
    "    \n",
    "    return ' '.join(translated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4917e91-fb01-459a-b403-981f0a54cedb",
   "metadata": {},
   "source": [
    "## Building the User Interface (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13bfefa1-1d27-48ad-b8b3-fcd7bd02a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GUI\n",
    "def create_interface():\n",
    "    def translate():\n",
    "        sentence = input_sentence.get()\n",
    "        src_lang = source_language_combobox.get()\n",
    "        tgt_lang = target_language_combobox.get()\n",
    "        \n",
    "        if not sentence or not src_lang or not tgt_lang:\n",
    "            messagebox.showerror(\"Error\", \"Please fill all fields!\")\n",
    "            return\n",
    "        \n",
    "        if src_lang == tgt_lang:\n",
    "            messagebox.showerror(\"Error\", \"Source and Target languages cannot be the same!\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            translated_sentence = translate_sentence(sentence, src_lang, tgt_lang)\n",
    "            result_label.config(text=f\"Translated Sentence: {translated_sentence}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Translation failed: {str(e)}\")\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Multilingual Translator\")\n",
    "    root.geometry(\"700x500\")\n",
    "    \n",
    "    # Title\n",
    "    tk.Label(root, text=\"Multilingual Translator\", font=(\"Arial\", 16, \"bold\")).pack(pady=10)\n",
    "    \n",
    "    # Source Language\n",
    "    tk.Label(root, text=\"Source Language:\", font=(\"Arial\", 12)).pack(pady=5)\n",
    "    source_language_combobox = ttk.Combobox(root, values=languages, state=\"readonly\")\n",
    "    source_language_combobox.pack(pady=5)\n",
    "    source_language_combobox.set(source_language)\n",
    "    \n",
    "    # Target Language\n",
    "    tk.Label(root, text=\"Target Language:\", font=(\"Arial\", 12)).pack(pady=5)\n",
    "    target_language_combobox = ttk.Combobox(root, values=languages, state=\"readonly\")\n",
    "    target_language_combobox.pack(pady=5)\n",
    "    target_language_combobox.set(target_language)\n",
    "    \n",
    "    # Input Sentence\n",
    "    tk.Label(root, text=\"Enter Sentence:\", font=(\"Arial\", 12)).pack(pady=5)\n",
    "    input_sentence = tk.Entry(root, width=50, font=(\"Arial\", 12))\n",
    "    input_sentence.pack(pady=5)\n",
    "    \n",
    "    # Translate Button\n",
    "    tk.Button(root, text=\"Translate\", font=(\"Arial\", 12), command=translate).pack(pady=10)\n",
    "    \n",
    "    # Translation Result\n",
    "    result_label = tk.Label(root, text=\"Translated Sentence: \", font=(\"Arial\", 12), wraplength=600, justify=\"left\")\n",
    "    result_label.pack(pady=10)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics_label = tk.Label(\n",
    "        root,\n",
    "        text=f\"Training Loss: {final_train_loss:.4f}, Training Accuracy: {final_train_accuracy:.4f}\\n\"\n",
    "             f\"Validation Loss: {final_val_loss:.4f}, Validation Accuracy: {final_val_accuracy:.4f}\",\n",
    "        font=(\"Arial\", 12),\n",
    "        justify=\"left\",\n",
    "        wraplength=600\n",
    "    )\n",
    "    metrics_label.pack(pady=10)\n",
    "    \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eaf75-a386-40ed-91eb-7e99acb4799e",
   "metadata": {},
   "source": [
    "## Launching the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261abdb-745b-4b0d-beae-336a75280b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the GUI\n",
    "create_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27da66d-8813-431d-9626-2e3a68714589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397b3d0-535f-4273-88a7-81d08d94e20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
